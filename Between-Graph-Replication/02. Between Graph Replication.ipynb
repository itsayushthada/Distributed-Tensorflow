{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Between-graph replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: (60000, 28, 28)\n",
      "Test Set: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Set: {}\".format(x_train.shape))\n",
    "print(\"Test Set: {}\".format(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: (60000,)\n",
      "Test Set: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Set: {}\".format(y_train.shape))\n",
    "print(\"Test Set: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.eye(10)[y_train]\n",
    "y_test = np.eye(10)[y_test]\n",
    "\n",
    "if y_train.shape[1] != 10:\n",
    "    y_train = y_train[:,0]\n",
    "    y_test = y_test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: (60000, 10)\n",
      "Test Set: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Set: {}\".format(y_train.shape))\n",
    "print(\"Test Set: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = tf.train.ClusterSpec(\n",
    "    {\n",
    "        \"ps\": [\"172.17.0.2:2223\",\n",
    "               \"172.17.0.3:2224\"],\n",
    "        \"worker\": [\"172.17.0.1:2222\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 28, 28], name=\"x\")\n",
    "y = tf.placeholder(tf.int8, shape=[None, 10], name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub Graph on Parameter Server 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/job:ps/task:0\"):  \n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "                              inputs=input_layer,\n",
    "                              filters=32,\n",
    "                              kernel_size=[5, 5],\n",
    "                              padding=\"same\",\n",
    "                              activation=tf.nn.relu\n",
    "                            )\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(\n",
    "                                    inputs=conv1, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2\n",
    "                                    )\n",
    "\n",
    "    # Convolutional Layer #2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "                              inputs=pool1,\n",
    "                              filters=64,\n",
    "                              kernel_size=[5, 5],\n",
    "                              padding=\"same\",\n",
    "                              activation=tf.nn.relu\n",
    "                            )\n",
    "    \n",
    "    # Pooling Layer #2\n",
    "    pool2 = tf.layers.max_pooling2d(\n",
    "                                    inputs=conv2, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub Graph on Parameter Server 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/job:ps/task:1\"):\n",
    "    # Dense Layer\n",
    "    pool2_flat = tf.reshape(\n",
    "                            tensor=pool2, \n",
    "                            shape=[-1, 7 * 7 * 64]\n",
    "                            )\n",
    "    \n",
    "    dense = tf.layers.dense(\n",
    "                            inputs=pool2_flat, \n",
    "                            units=1024, \n",
    "                            activation=tf.nn.relu\n",
    "                            )\n",
    "    \n",
    "    dropout = tf.layers.dropout(\n",
    "                                inputs=dense, \n",
    "                                rate=0.4, \n",
    "                                )\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(\n",
    "                            inputs=dropout, \n",
    "                            units=10\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub Graph on Worker Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/job:worker/task:0\"): \n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=tf.argmax(y, axis=1), logits=logits)\n",
    "\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(\n",
    "                                loss=loss,\n",
    "                                global_step=tf.train.get_global_step()\n",
    "                              )\n",
    "\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5000\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:    0   Loss:2.327526   Val_Acc:0.000%   Test_Acc:12.120%\n",
      "Epochs:  100   Loss:2.292347   Val_Acc:18.750%   Test_Acc:20.800%\n",
      "Epochs:  200   Loss:2.253204   Val_Acc:31.250%   Test_Acc:29.460%\n",
      "Epochs:  300   Loss:2.251566   Val_Acc:43.750%   Test_Acc:38.220%\n",
      "Epochs:  400   Loss:2.218656   Val_Acc:31.250%   Test_Acc:44.970%\n",
      "Epochs:  500   Loss:2.236017   Val_Acc:31.250%   Test_Acc:52.180%\n",
      "Epochs:  600   Loss:2.205187   Val_Acc:37.500%   Test_Acc:57.170%\n",
      "Epochs:  700   Loss:2.097551   Val_Acc:62.500%   Test_Acc:60.250%\n",
      "Epochs:  800   Loss:2.106690   Val_Acc:43.750%   Test_Acc:59.100%\n",
      "Epochs:  900   Loss:1.979638   Val_Acc:62.500%   Test_Acc:63.400%\n",
      "Epochs: 1000   Loss:1.865477   Val_Acc:81.250%   Test_Acc:68.010%\n",
      "Epochs: 1100   Loss:1.688928   Val_Acc:75.000%   Test_Acc:74.300%\n",
      "Epochs: 1200   Loss:1.665369   Val_Acc:68.750%   Test_Acc:77.260%\n",
      "Epochs: 1300   Loss:1.516363   Val_Acc:68.750%   Test_Acc:75.700%\n",
      "Epochs: 1400   Loss:1.198220   Val_Acc:87.500%   Test_Acc:79.090%\n",
      "Epochs: 1500   Loss:1.197136   Val_Acc:62.500%   Test_Acc:80.280%\n",
      "Epochs: 1600   Loss:0.937675   Val_Acc:81.250%   Test_Acc:81.130%\n",
      "Epochs: 1700   Loss:0.796196   Val_Acc:81.250%   Test_Acc:83.740%\n",
      "Epochs: 1800   Loss:0.624401   Val_Acc:81.250%   Test_Acc:84.230%\n",
      "Epochs: 1900   Loss:0.942445   Val_Acc:68.750%   Test_Acc:84.760%\n",
      "Epochs: 2000   Loss:0.531413   Val_Acc:81.250%   Test_Acc:84.530%\n",
      "Epochs: 2100   Loss:0.552047   Val_Acc:87.500%   Test_Acc:86.390%\n",
      "Epochs: 2200   Loss:0.740905   Val_Acc:75.000%   Test_Acc:87.730%\n",
      "Epochs: 2300   Loss:0.287011   Val_Acc:93.750%   Test_Acc:87.920%\n",
      "Epochs: 2400   Loss:0.308646   Val_Acc:93.750%   Test_Acc:88.180%\n",
      "Epochs: 2500   Loss:0.420506   Val_Acc:87.500%   Test_Acc:88.760%\n",
      "Epochs: 2600   Loss:0.220915   Val_Acc:87.500%   Test_Acc:89.020%\n",
      "Epochs: 2700   Loss:0.326307   Val_Acc:93.750%   Test_Acc:88.750%\n",
      "Epochs: 2800   Loss:0.500016   Val_Acc:93.750%   Test_Acc:88.980%\n",
      "Epochs: 2900   Loss:0.322882   Val_Acc:93.750%   Test_Acc:89.630%\n",
      "Epochs: 3000   Loss:0.366505   Val_Acc:93.750%   Test_Acc:89.770%\n",
      "Epochs: 3100   Loss:0.177960   Val_Acc:100.000%   Test_Acc:89.740%\n",
      "Epochs: 3200   Loss:0.687373   Val_Acc:87.500%   Test_Acc:89.670%\n",
      "Epochs: 3300   Loss:0.372253   Val_Acc:87.500%   Test_Acc:90.030%\n",
      "Epochs: 3400   Loss:0.376482   Val_Acc:93.750%   Test_Acc:90.760%\n",
      "Epochs: 3500   Loss:0.172471   Val_Acc:100.000%   Test_Acc:91.000%\n",
      "Epochs: 3600   Loss:0.065900   Val_Acc:100.000%   Test_Acc:90.100%\n",
      "Epochs: 3700   Loss:0.468824   Val_Acc:93.750%   Test_Acc:91.060%\n",
      "Epochs: 3800   Loss:0.188265   Val_Acc:93.750%   Test_Acc:91.010%\n",
      "Epochs: 3900   Loss:0.595088   Val_Acc:81.250%   Test_Acc:91.110%\n",
      "Epochs: 4000   Loss:0.346501   Val_Acc:87.500%   Test_Acc:91.540%\n",
      "Epochs: 4100   Loss:0.442263   Val_Acc:81.250%   Test_Acc:91.410%\n",
      "Epochs: 4200   Loss:0.086872   Val_Acc:100.000%   Test_Acc:91.290%\n",
      "Epochs: 4300   Loss:0.238630   Val_Acc:93.750%   Test_Acc:91.980%\n",
      "Epochs: 4400   Loss:0.060559   Val_Acc:100.000%   Test_Acc:91.960%\n",
      "Epochs: 4500   Loss:0.114969   Val_Acc:100.000%   Test_Acc:91.570%\n",
      "Epochs: 4600   Loss:0.207466   Val_Acc:93.750%   Test_Acc:91.520%\n",
      "Epochs: 4700   Loss:0.388476   Val_Acc:93.750%   Test_Acc:91.960%\n",
      "Epochs: 4800   Loss:0.381983   Val_Acc:81.250%   Test_Acc:92.340%\n",
      "Epochs: 4900   Loss:0.057706   Val_Acc:100.000%   Test_Acc:92.320%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(\"grpc://172.17.0.1:2222\") as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(EPOCHS):\n",
    "        idx = np.random.randint(0, x_train.shape[0], size=BATCH_SIZE)\n",
    "        \n",
    "        x_batch = x_train[idx]\n",
    "        y_batch = y_train[idx]\n",
    "        \n",
    "        sess.run(train_op, feed_dict={ x: x_batch, y: y_batch })\n",
    "        LOSS, ACC = sess.run([loss, accuracy], feed_dict={ x: x_batch, y: y_batch })\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            TEST_ACC = 0\n",
    "            count = 0\n",
    "            j=0\n",
    "            while j<x_test.shape[0] :\n",
    "                TEST_ACC += sess.run(accuracy, feed_dict={ \n",
    "                                x: x_test[j:min(j+BATCH_SIZE, x_test.shape[0])], \n",
    "                                y: y_test[j:min(j+BATCH_SIZE, x_test.shape[0])]\n",
    "                                                        })\n",
    "\n",
    "                j = j+BATCH_SIZE\n",
    "                count += 1\n",
    "                \n",
    "            print(\"Epochs: {:4d}   Loss:{:.6f}   Val_Acc:{:.3f}%   Test_Acc:{:.3f}%\".format(i, LOSS, ACC*100, 100*TEST_ACC/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reinforcment",
   "language": "python",
   "name": "reinforcment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
